time_step: 0.01
lr: 0.001
weight_decay: 1e-05
num_epoch: 5
num_train: 1000
num_test: 1000
num_val: 0
num_trans: 0
batch_size: 10
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
optim_name: AdamW
n_hidden: 128
n_layers: 3
reg_param: 500
train_dir: ../plot/Vector_field/
Epoch 0: New minimal relative error: 3.04%, model saved.
Epoch: 0 Train: 15.61319 Test: 3.03840
Epoch 1: New minimal relative error: 0.47%, model saved.
Epoch: 1 Train: 0.20680 Test: 0.47233
Epoch 2: New minimal relative error: 0.44%, model saved.
Epoch: 2 Train: 0.07302 Test: 0.44408
Epoch 3: New minimal relative error: 0.44%, model saved.
Epoch: 3 Train: 0.05337 Test: 0.44309
Epoch 4: New minimal relative error: 0.40%, model saved.
Epoch: 4 Train: 0.03024 Test: 0.39762
Training Loss: tensor(0.0019)
Test Loss: tensor(-0.0499)
Learned LE: [-45.138325 -61.223663 -82.1944  ]
True LE: [  0.6696434    0.02661083 -14.418215  ]
Learned mean: tensor([0.1354, 0.0984, 0.3124], device='cuda:0', grad_fn=<MeanBackward1>)
True mean: tensor([-3.9267, -3.9181, 24.3427])
Learned variance: tensor([2.6042e-04, 4.2532e-05, 1.8118e-04], device='cuda:0',
       grad_fn=<VarBackward0>)
True variance: tensor([49.7680, 64.6174, 60.8852])
