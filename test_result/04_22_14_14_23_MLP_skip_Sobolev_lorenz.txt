time_step: 0.01
lr: 0.001
weight_decay: 1e-05
num_epoch: 500
num_train: 5000
num_test: 1000
num_val: 0
num_trans: 0
batch_size: 10
loss_type: Sobolev
dyn_sys: lorenz
model_type: MLP_skip
optim_name: AdamW
n_hidden: 128
n_layers: 3
reg_param: 500
train_dir: ../plot/Vector_field/
Epoch 0: New minimal relative error: 0.68%, model saved.
Epoch: 0 Train: 3.48738 Test: 0.67634
Epoch 100: New minimal relative error: 0.01%, model saved.
Epoch: 100 Train: 0.01187 Test: 0.01102
Epoch 200: New minimal relative error: 0.00%, model saved.
Epoch: 200 Train: 0.00635 Test: 0.00375
Epoch 300: New minimal relative error: 0.00%, model saved.
Epoch: 300 Train: 0.00270 Test: 0.00093
Epoch 400: New minimal relative error: 0.00%, model saved.
Epoch: 400 Train: 0.00207 Test: 0.00042
Epoch 499: New minimal relative error: 0.00%, model saved.
Epoch: 499 Train: 0.00122 Test: 0.00019
Training Loss: tensor(0.0012)
Test Loss: tensor(0.0002)
Learned LE: [ -7.5282545  -7.602126  -18.926418 ]
True LE: [ 7.4157292e-01  1.1452072e-04 -1.4449540e+01]
Learned mean: tensor([14.8849, -7.6123,  5.9722], device='cuda:0', grad_fn=<MeanBackward1>)
True mean: tensor([-1.6414, -1.6663, 24.1288])
Learned variance: tensor([0.7374, 0.1076, 0.5435], device='cuda:0', grad_fn=<VarBackward0>)
True variance: tensor([62.1465, 77.2169, 65.5227])
